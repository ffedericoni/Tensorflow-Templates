{
  "cells": [
    {
      "metadata": {
        "_uuid": "c6405a9ba97dc7bfae5437fea9ae4943ccd0cb55"
      },
      "cell_type": "raw",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport shutil\nimport pandas as pd\n\nprint(tf.__version__)\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# List the CSV columns\nCSV_COLUMNS = ['fare_amount', 'pickup_datetime','pickup_longitude','pickup_latitude',\n               'dropoff_longitude','dropoff_latitude', 'passenger_count', 'key']\n\n#Choose which column is your label\nLABEL_COLUMN = 'fare_amount'\nimport os\nprint(os.listdir(\"../input\"))\n\n#TODO create two separate datasets for Training and Evaluation",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d316db9def3e86baeb1830b6dd70a47e843aa5c"
      },
      "cell_type": "code",
      "source": "#This is just to have a look at the data\nPATH = '../input'\ntrain_df = pd.read_csv(f'{PATH}/train.csv', nrows=10000)\ntrain_df['distance'] = np.sqrt(np.abs(train_df['pickup_longitude']-train_df['dropoff_longitude'])**2 +\n                        np.abs(train_df['pickup_latitude']-train_df['dropoff_latitude'])**2)\ntrain_df.head()\ntrain_df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "973ceacc4cd789f6ad067e6b2842e7b7898828a1"
      },
      "cell_type": "code",
      "source": "BATCH_SIZE=8\ndataset = tf.contrib.data.make_csv_dataset(\n    file_pattern=f'{PATH}/train.csv',\n    batch_size=BATCH_SIZE,\n    column_names=None,\n    column_defaults=None,\n    label_name='fare_amount',\n    select_columns=[1, 2, 3, 4, 5, 6, 7],\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=1,\n    num_parallel_reads=1,\n    num_parallel_parser_calls=2,\n    sloppy=False,\n    num_rows_for_inference=100\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9a4c231b1f84135c399539731e2f0f349aabc13"
      },
      "cell_type": "code",
      "source": "next_element = dataset.make_one_shot_iterator().get_next()\nwith tf.Session() as sess:\n    features, label = sess.run(next_element)\n    print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "227a11c9cb10a34d769700835e9aaf1450e356f0"
      },
      "cell_type": "code",
      "source": "def pd_weekDay(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.weekday.astype(np.int32)\n\nyears=np.array([2018, 2018, 2018])\nmonths=np.array([8, 11, 1])\ndays=np.array([20, 6, 8])\nprint(pd_weekDay(years, months, days))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6951ccbd7cb40912a67a66b0ef1aa3043fcc399c"
      },
      "cell_type": "code",
      "source": "def tf_isAirport(latitude,longitude,airport_name='JFK'):\n    jfkcoord = tf.constant([-73.8352, -73.7401, 40.6195, 40.6659])\n    if airport_name=='JFK':\n        coord = jfkcoord\n    is_airport = \\\n    tf.logical_and(\n        tf.logical_and(\n            tf.greater(latitude, coord[0]), tf.less(latitude, coord[1])\n        ),\n        tf.logical_and(\n            tf.greater(longitude, coord[2]), tf.less(longitude, coord[3])\n        )\n    )\n    return is_airport",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "930d90d91583479c0613eb91f8150e02ddf525ec"
      },
      "cell_type": "code",
      "source": "def feat_eng_func(features, label=None):\n    print(\"Feature Engineered Label:\", label)\n    #New features based on pickup datetime\n    features['pickup_year'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 0, 4), tf.int32)\n    features['pickup_month'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 5, 2), tf.int32)\n    features['pickup_day'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 8, 2), tf.int32)\n    features['pickup_hour'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 11, 2), tf.int32)\n    features['pickup_weekday'] = tf.py_func(pd_weekDay,\n                                            [features['pickup_year'], features['pickup_month'], features['pickup_day']],\n                                            tf.int32,\n                                            stateful=False,\n                                            name='Weekday'\n                                           )\n    #Normalize year and add decimals for months. This is because fares increase with time\n    features['pickup_dense_year'] = (\n                tf.cast(features['pickup_year'], tf.float32) + \\\n                tf.cast(features['pickup_month'], tf.float32) / tf.constant(12.0, tf.float32) -  \\\n                 tf.constant(2009.0, tf.float32) ) /  \\\n                 tf.constant(6.0, tf.float32) \n   \n    #Clip latitudes and longitudes\n    minlat = tf.constant(40.0)\n    maxlat = tf.constant(42.0)\n    minlon = tf.constant(-75.0)\n    maxlon = tf.constant(-72.0)\n    features['pickup_longitude'] = tf.clip_by_value(features['pickup_longitude'], minlon, maxlon)\n    features['pickup_latitude'] = tf.clip_by_value(features['pickup_latitude'], minlat, maxlat)\n    features['dropoff_longitude'] = tf.clip_by_value(features['dropoff_longitude'], minlon, maxlon)\n    features['dropoff_latitude'] = tf.clip_by_value(features['dropoff_latitude'], minlat, maxlat)\n    #TODO feature for the day of the week\n    #New features based on pickup and dropoff position\n    features['longitude_dist'] = tf.abs(features['pickup_longitude'] - features['dropoff_longitude'])\n    features['latitude_dist'] = tf.abs(features['pickup_latitude'] - features['dropoff_latitude'])\n    #compute euclidean distance of the trip\n    features['distance'] = tf.sqrt(features['longitude_dist']**2 + features['latitude_dist']**2)\n    long_distance = tf.constant(0.07)\n    features['is_long_distance'] = tf.less(long_distance, features['distance'])\n    features['is_JFK_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='JFK')\n    features['is_JFK_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='JFK')\n#    features['pickup_minute'] = tf.substr(features['pickup_datetime'], 14, 2)\n#TODO normalize long and lat\n#TODO remove outliers on passenger_count and fare_amount\n    print(features)\n    if label == None:\n        return features\n    return (features, label)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an input function that stores your data into a dataset\ndef read_dataset(filename, mode, batch_size = 512):\n    def _input_fn():    \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            shuffle = True\n        else:\n            num_epochs = 1 # end-of-input after this\n            shuffle = False\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            label_name=None\n            select_columns=[1, 2, 3, 4, 5, 6]\n        else:\n            label_name ='fare_amount'\n            select_columns = [1, 2, 3, 4, 5, 6, 7]\n\n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(filename)\n        # Create Dataset from the CSV files\n        dataset = tf.contrib.data.make_csv_dataset(\n            file_pattern=file_list,\n            batch_size=batch_size,\n            column_names=None,\n            column_defaults=None,\n            label_name=label_name,\n            select_columns=select_columns,\n            field_delim=',',\n            use_quote_delim=True,\n            na_value='',\n            header=True,\n            num_epochs=num_epochs,\n            shuffle=shuffle,\n            shuffle_buffer_size=128*batch_size,\n            shuffle_seed=None,\n            prefetch_buffer_size=1,\n            num_parallel_reads=1,\n            num_parallel_parser_calls=3,\n            sloppy=False,\n            num_rows_for_inference=100\n        )\n        train_lines = 55000000 // batch_size // 100 * 80\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            dataset = dataset.take(train_lines)\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            dataset = dataset.skip(train_lines) #EVAL on different data\n\n        dataset = dataset.map(feat_eng_func)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d06e3f13a201c8dbcb80a8069abbf5f6b8930776"
      },
      "cell_type": "code",
      "source": "train_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = 8)\nnext_element = train_input_fn()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1995703313676a2b5bf642fa2a8874017f260a8",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    features, label = sess.run(train_input_fn())\n    print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35fc0403cde57b11b4fa0a00879b842ca634e328"
      },
      "cell_type": "code",
      "source": "# Define your feature columns\ndef create_feature_cols():\n    hour_cat = tf.feature_column.categorical_column_with_identity('pickup_hour', 24 )\n    weekday_cat = tf.feature_column.categorical_column_with_identity('pickup_weekday', 7)\n    hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n\n    return [\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    #TODO use pickup_year\n    tf.feature_column.numeric_column('pickup_dense_year'),\n#    tf.feature_column.numeric_column('pickup_year'),\n#    tf.feature_column.numeric_column('pickup_month'),\n#    tf.feature_column.numeric_column('pickup_day'),\n    #TODO use embeddings for the hour\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_hour', (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n    #                                                                        11, 12, 13, 14, 15, 16, 17, 18,\n    #                                                                         19, 20, 21, 22, 23) )\n    #                                  ),\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_weekday', (0, 1, 2, 3, 4, 5, 6)\n    #                                                                                            )),\n    tf.feature_column.embedding_column(hour_X_weekday, 2),\n    tf.feature_column.numeric_column('longitude_dist'),\n    tf.feature_column.numeric_column('latitude_dist'),\n    tf.feature_column.numeric_column('distance'),\n    tf.feature_column.numeric_column('is_JFK_pickup'),\n    tf.feature_column.numeric_column('is_JFK_dropoff'),\n#    tf.feature_column.numeric_column('is_long_distance')\n  ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5607f77fb81893225f8e731f04d90919d00d2fd9"
      },
      "cell_type": "code",
      "source": "BATCH_SIZE = 512\ntrain_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\n# Create estimator train and evaluate function\ndef train_and_evaluate(output_dir, num_train_steps):\n#    estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n    estimator = tf.estimator.DNNRegressor(model_dir = output_dir, feature_columns = create_feature_cols(),\n                                         hidden_units=[32, 32, 16])\n    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn, \n                                      max_steps = num_train_steps)\n    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn, \n                                    steps = None, \n                                    start_delay_secs = 1, # start evaluating after N seconds, \n                                    throttle_secs = 60)  # evaluate every N seconds\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n    return estimator\n    \n\nOUTDIR = './trained_model'\nshutil.rmtree(OUTDIR, ignore_errors = True)\n#model = train_and_evaluate(OUTDIR, NUM_STEPS)\n#print(\"BATCH SIZE = \", BATCH_SIZE,\"\\nDataset Take = \", 128*BATCH_SIZE)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7394b8ab69057c7131c3761c08e82f923bb8c5ee",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "BATCH_SIZE = 256\nOUTDIR = './trained_model'\ntrain_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\nshutil.rmtree(OUTDIR, ignore_errors = True)\n#estimator = tf.estimator.LinearRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols())\nestimator = tf.estimator.DNNRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols(),\n                                     hidden_units=[128, 64, 32])\nestimator.train(train_input_fn, max_steps=40000)\nestimator.evaluate(eval_input_fn, name='train_eval')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0f0b1b2aa59ecaca8479a212fb3bb6859d0a7b1",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "predict_input_fn = read_dataset(f'{PATH}/test.csv', tf.estimator.ModeKeys.PREDICT, batch_size=1)\npredictions = estimator.predict(predict_input_fn)\n\ns = pd.Series()\nfor i, p in enumerate(predictions):\n    if i < 10000:\n        s.at[i] = p['predictions'][0]\n    else:\n        break\ns.describe()\ns.to_csv(\"DNNregr.csv\")\n#    print(\"Prediction %s: %s\" % (i + 1, p))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "461348d3e057332516dbf2f65978bf1008e8f3f0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def weekDay(year, month, day):\n    offset = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n    week   = ['Sunday', \n              'Monday', \n              'Tuesday', \n              'Wednesday', \n              'Thursday',  \n              'Friday', \n              'Saturday']\n    afterFeb = 1\n    if month > 2: afterFeb = 0\n    aux = year - 1700 - afterFeb\n    # dayOfWeek for 1700/1/1 = 5, Friday\n    dayOfWeek  = 5\n    # partial sum of days betweem current date and 1700/1/1\n    dayOfWeek += (aux + afterFeb) * 365                  \n    # leap year correction    \n    dayOfWeek += aux // 4 - aux // 100 + (aux + 100) // 400     \n    # sum monthly and day offsets\n    dayOfWeek += offset[month - 1] + (day - 1)               \n    dayOfWeek %= 7\n    return dayOfWeek, week[dayOfWeek]\n\nprint(weekDay(2018, 8, 17))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "424ec19726fd2aff6f4da3d3d5f2621bc3220869"
      },
      "cell_type": "code",
      "source": "nyc_airports={'JFK':{'min_lng':-73.8352,\n     'min_lat':40.6195,\n     'max_lng':-73.7401, \n     'max_lat':40.6659},\n              \n    'EWR':{'min_lng':-74.1925,\n            'min_lat':40.6700, \n            'max_lng':-74.1531, \n            'max_lat':40.7081\n\n        },\n    'LaGuardia':{'min_lng':-73.8895, \n                  'min_lat':40.7664, \n                  'max_lng':-73.8550, \n                  'max_lat':40.7931\n        \n    }\n    \n}\n\ndef isAirport(latitude,longitude,airport_name='JFK'):\n    if latitude>=nyc_airports[airport_name]['min_lat'] and \n        latitude<=nyc_airports[airport_name]['max_lat'] and \n        longitude>=nyc_airports[airport_name]['min_lng'] and \n        longitude<=nyc_airports[airport_name]['max_lng']:\n        return 1\n    else:\n        return 0\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3781e17f737b6f502b4f2c7fc754654e8c09d4e"
      },
      "cell_type": "code",
      "source": "def tf_isAirport(latitude,longitude,airport_name='JFK'):\n    jfkcoord = tf.constant([-73.8352, -73.7401, 40.6195, 40.6659])\n    if airport_name=='JFK':\n        coord = jfkcoord\n    is_airport = \\\n    tf.logical_and(\n        tf.logical_and(\n            tf.greater(latitude, coord[0]), tf.less(latitude, coord[1])\n        ),\n        tf.logical_and(\n            tf.greater(longitude, coord[2]), tf.less(longitude, coord[3])\n        )\n    )\n    return is_airport\n\nisair = tf_isAirport(-73.8342, 40.62)\nwith tf.Session() as sess:\n    output = sess.run(isair)\n    print(output)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9fd1ffca2eafc53bc3bf01c10363079be19ea65"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}