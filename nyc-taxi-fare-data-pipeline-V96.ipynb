{"cells":[{"metadata":{"_uuid":"c6405a9ba97dc7bfae5437fea9ae4943ccd0cb55"},"cell_type":"raw","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os; os.environ['OMP_NUM_THREADS'] = '4'\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport shutil\nimport pandas as pd\n\nprint(tf.__version__)\nassert tf.__version__ >= \"1.8\" or tf.__version__ >= \"1.10\"\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# List the CSV columns\nCSV_COLUMNS = ['fare_amount', 'pickup_datetime','pickup_longitude','pickup_latitude',\n               'dropoff_longitude','dropoff_latitude', 'passenger_count', 'key']\n\n#Choose which column is your label\nLABEL_COLUMN = 'fare_amount'\nTRAIN_LINES = 55423856\n#import os\n#print(os.listdir(\"../input\"))\n\n#TODO create two separate datasets for Training and Evaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41690bb0b2e4fe9a2429efd13f517c194d1641e2"},"cell_type":"code","source":"from contextlib import contextmanager\nimport time\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d316db9def3e86baeb1830b6dd70a47e843aa5c"},"cell_type":"code","source":"#This is just to have a look at the data\nPATH = '../input'\ntrain_df = pd.read_csv(f'{PATH}/train.csv', nrows=10000)\ntrain_df['distance'] = np.sqrt(np.abs(train_df['pickup_longitude']-train_df['dropoff_longitude'])**2 +\n                        np.abs(train_df['pickup_latitude']-train_df['dropoff_latitude'])**2)\ntrain_df.head()\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"973ceacc4cd789f6ad067e6b2842e7b7898828a1"},"cell_type":"code","source":"BATCH_SIZE=1 #Filtering works only with size 1 batches!!\ndataset = tf.contrib.data.make_csv_dataset(\n    file_pattern=f'{PATH}/train.csv',\n    batch_size=BATCH_SIZE,\n    column_names=None,\n    column_defaults=None,\n    label_name='fare_amount',\n    select_columns=[1, 2, 3, 4, 5, 6, 7],\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=1,\n    num_parallel_reads=1,\n    num_parallel_parser_calls=2,\n    sloppy=False,\n    num_rows_for_inference=100\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9a4c231b1f84135c399539731e2f0f349aabc13"},"cell_type":"code","source":"def filter_data(features, label=None):\n#    label = tf.Print(label, [label], \"Filtering...\", first_n=1,)\n    val = tf.greater(label , tf.constant(20.0, tf.float32))\n    return True\n\ndataset = dataset.filter(filter_data)\nnext_element = dataset.make_one_shot_iterator().get_next()\nwith tf.Session() as sess:\n    features, label = sess.run(next_element)\n    print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227a11c9cb10a34d769700835e9aaf1450e356f0"},"cell_type":"code","source":"def pd_weekDay(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.weekday.astype(np.int32)\n\ndef pd_dayofYear(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.dayofyear.astype(np.int32)\n\nyears=np.array([2018, 2018, 2018])\nmonths=np.array([8, 11, 1])\ndays=np.array([20, 6, 8])\nprint(pd_dayofYear(years, months, days))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6951ccbd7cb40912a67a66b0ef1aa3043fcc399c"},"cell_type":"code","source":"def tf_isAirport(latitude,longitude,airport_name='JFK'):\n    jfkcoord = tf.constant([-73.8352, -73.7401, 40.6195, 40.6659])\n    ewrcoord = tf.constant([-74.1925, -74.1531, 40.6700, 40.7081])\n    lgucoord = tf.constant([-73.8895, -73.8550, 40.7664, 40.7931])\n    if airport_name=='JFK':\n        coord = jfkcoord\n    elif airport_name=='EWR':\n        coord = ewrcoord\n    elif airport_name=='LGU':\n        coord = lgucoord\n    else:\n        raise ValueError( f'Unknown NYC Airport {airport_name}' )\n        \n    is_airport = \\\n    tf.logical_and(\n        tf.logical_and(\n            tf.greater(latitude, coord[0]), tf.less(latitude, coord[1])\n        ),\n        tf.logical_and(\n            tf.greater(longitude, coord[2]), tf.less(longitude, coord[3])\n        )\n    )\n    return is_airport\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"930d90d91583479c0613eb91f8150e02ddf525ec"},"cell_type":"code","source":"def feat_eng_func(features, label=None):\n    print(\"Feature Engineered Label:\", label)\n    #New features based on pickup datetime\n    features['pickup_year'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 0, 4), tf.int32)\n    features['pickup_month'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 5, 2), tf.int32)\n    features['pickup_day'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 8, 2), tf.int32)\n    features['pickup_hour'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 11, 2), tf.int32)\n    #TODO is there an easy way to perform below functions using TF APIs?\n    features['pickup_weekday'] = tf.py_func(pd_weekDay,\n                                            [features['pickup_year'], features['pickup_month'], features['pickup_day']],\n                                            tf.int32,\n                                            stateful=False,\n                                            name='Weekday'\n                                           )\n    #no advantage features['pickup_dayofyear'] = tf.cast(features['pickup_month'] * 31 + features['pickup_day'], tf.int32 ) #not precise, but good enough\n    #Normalize year and add decimals for months. This is because fares increase with time\n    features['pickup_dense_year'] = (\n                tf.cast(features['pickup_year'], tf.float32) + \\\n                tf.cast(features['pickup_month'], tf.float32) / tf.constant(12.0, tf.float32) -  \\\n                 tf.constant(2009.0, tf.float32) ) /  \\\n                 tf.constant(6.0, tf.float32) \n   \n    #Clip latitudes and longitudes\n    minlat = tf.constant(38.0)\n    maxlat = tf.constant(42.0)\n    minlon = tf.constant(-76.0)\n    maxlon = tf.constant(-72.0)\n    features['pickup_longitude'] = tf.clip_by_value(features['pickup_longitude'], minlon, maxlon)\n    features['pickup_latitude'] = tf.clip_by_value(features['pickup_latitude'], minlat, maxlat)\n    features['dropoff_longitude'] = tf.clip_by_value(features['dropoff_longitude'], minlon, maxlon)\n    features['dropoff_latitude'] = tf.clip_by_value(features['dropoff_latitude'], minlat, maxlat)\n    #Clip (normalize passengers didn't work?)\n    minpass = tf.constant(1.0)\n    maxpass = tf.constant(6.0)\n    features['passenger_count'] = tf.clip_by_value(tf.cast(features['passenger_count'], tf.float32), minpass, maxpass)\n    #Clip fare_amount\n    #TODO normalize or tf.log the fare_amount\n    if label != None:\n        minfare = tf.constant(1.0)\n        maxfare = tf.constant(300.0)\n        label = tf.clip_by_value(label,  minfare, maxfare) \n    #TODO feature for bridge passing\n    #New features based on pickup and dropoff position\n    features['longitude_dist'] = tf.abs(features['pickup_longitude'] - features['dropoff_longitude'])\n    features['latitude_dist'] = tf.abs(features['pickup_latitude'] - features['dropoff_latitude'])\n    #compute euclidean distance of the trip (multiply by 10 to slightly normalize)\n    features['distance'] = tf.sqrt(features['longitude_dist']**2 + features['latitude_dist']**2)\n#    long_distance = tf.constant(0.7)\n#    features['is_long_distance'] = tf.less(long_distance, features['distance'])\n    features['is_JFK_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='JFK')\n    features['is_JFK_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='JFK')\n    features['is_EWR_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='EWR')\n    features['is_EWR_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='EWR')\n    features['is_LGU_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='LGU')\n    features['is_LGU_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='LGU')\n    features['is_NYC_airport'] = tf.logical_or(\n        tf.logical_or(\n            tf.logical_or(features['is_JFK_pickup'], features['is_JFK_dropoff']),\n            tf.logical_or(features['is_EWR_pickup'], features['is_EWR_dropoff'])),\n        tf.logical_or(features['is_LGU_pickup'], features['is_LGU_dropoff'])\n    )\n    \n#    features['pickup_minute'] = tf.substr(features['pickup_datetime'], 14, 2)\n#TODO normalize long and lat\n#TODO remove outliers on passenger_count and fare_amount\n#    print(features)\n    if label == None:\n        return features\n    return (features, label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Create an input function that stores your data into a dataset\ndef read_dataset(filename, mode, batch_size = 512):\n    def _input_fn():    \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            shuffle = False\n        else:\n            num_epochs = 1 # end-of-input after this\n            shuffle = False\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            label_name=None\n            select_columns=[1, 2, 3, 4, 5, 6]\n        else:\n            label_name ='fare_amount'\n            select_columns = [1, 2, 3, 4, 5, 6, 7]\n\n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(filename)\n        # Create Dataset from the CSV files\n        dataset = tf.contrib.data.make_csv_dataset(\n            file_pattern=file_list,\n            batch_size=batch_size, #for filtering\n            column_names=None,\n            column_defaults=None,\n            label_name=label_name,\n            select_columns=select_columns,\n            field_delim=',',\n            use_quote_delim=True,\n            na_value='',\n            header=True,\n            num_epochs=num_epochs,\n            shuffle=shuffle,\n            shuffle_buffer_size=128*batch_size,\n            shuffle_seed=None,\n            prefetch_buffer_size=1,\n            num_parallel_reads=1,\n            num_parallel_parser_calls=3,\n            sloppy=False,\n            num_rows_for_inference=100\n        )\n#This is necessary to split train and eval\n        skip_train_lines = TRAIN_LINES // batch_size // 100 * 10 #skip first 10% lines of train data set\n        if mode == tf.estimator.ModeKeys.TRAIN:\n#        dataset = dataset.filter(filter_data)\n            dataset = dataset.skip(skip_train_lines)\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            dataset = dataset.take(skip_train_lines) \n\n        dataset = dataset.map(feat_eng_func)\n#        dataset = dataset.repeat(3)\n#        dataset = dataset.batch(batch_size)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d06e3f13a201c8dbcb80a8069abbf5f6b8930776"},"cell_type":"code","source":"train_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = 8)\n#next_element = train_input_fn()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1995703313676a2b5bf642fa2a8874017f260a8"},"cell_type":"code","source":"with timer('Evaluating'):\n    with tf.Session() as sess:\n        features, label = sess.run(train_input_fn())\n        print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35fc0403cde57b11b4fa0a00879b842ca634e328"},"cell_type":"code","source":"# Define your feature columns\ndef create_feature_cols():\n    hour_cat = tf.feature_column.categorical_column_with_identity('pickup_hour', 24 )\n    weekday_cat = tf.feature_column.categorical_column_with_identity('pickup_weekday', 7)\n    hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n    days_list = range(367)\n    #yearday = tf.feature_column.categorical_column_with_vocabulary_list('pickup_dayofyear', days_list)\n    NUM_BUCKETS = 27\n    long_list = list(np.linspace(-74.2, -73.7, NUM_BUCKETS))\n    lat_list = list(np.linspace(40.5, 41.0, NUM_BUCKETS))\n    p_lon = tf.feature_column.numeric_column('pickup_longitude')\n    p_lat = tf.feature_column.numeric_column('pickup_latitude')\n    d_lon = tf.feature_column.numeric_column('dropoff_longitude')\n    d_lat = tf.feature_column.numeric_column('dropoff_latitude')\n    buck_p_lon = tf.feature_column.bucketized_column(p_lon, long_list)\n    buck_p_lat = tf.feature_column.bucketized_column(p_lat, lat_list)\n    buck_d_lon = tf.feature_column.bucketized_column(d_lon, long_list)\n    buck_d_lat = tf.feature_column.bucketized_column(d_lat, lat_list)\n    X_p = tf.feature_column.crossed_column([buck_p_lon, buck_p_lat], 3*NUM_BUCKETS**2)\n    X_d = tf.feature_column.crossed_column([buck_d_lon, buck_d_lat], 3*NUM_BUCKETS**2)\n    X_loc = tf.feature_column.crossed_column([X_p, X_d], (3*NUM_BUCKETS**2)**2 )\n#    minlat = tf.constant(38.0)\n#    maxlat = tf.constant(42.0)\n#    minlon = tf.constant(-76.0)\n#    maxlon = tf.constant(-72.0)\n    \n    return [\n    tf.feature_column.indicator_column(hour_cat),\n#    tf.feature_column.numeric_column('pickup_longitude'),\n#    tf.feature_column.numeric_column('pickup_latitude'),\n#    tf.feature_column.numeric_column('dropoff_longitude'),\n#    tf.feature_column.numeric_column('dropoff_latitude'),\n\n    tf.feature_column.numeric_column('passenger_count'),\n    tf.feature_column.numeric_column('pickup_dense_year'),\n#TODO    tf.feature_column.numeric_column('pickup_dayofyear'),\n#    tf.feature_column.embedding_column(yearday, 2),\n#    tf.feature_column.numeric_column('pickup_year'),\n#    tf.feature_column.numeric_column('pickup_month'),\n#    tf.feature_column.numeric_column('pickup_day'),\n    #TODO use embeddings for the hour\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_hour', (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n    #                                                                        11, 12, 13, 14, 15, 16, 17, 18,\n    #                                                                         19, 20, 21, 22, 23) )\n    #                                  ),\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_weekday', (0, 1, 2, 3, 4, 5, 6)\n    #                                                                                            )),\n    tf.feature_column.embedding_column(hour_X_weekday, 2),\n    tf.feature_column.embedding_column(\n        tf.feature_column.categorical_column_with_vocabulary_list('pickup_month', (0, 1, 2, 3, 4, 5, 6, \n                                                                                   7, 8, 9, 10, 11, 12)),\n        2),\n    tf.feature_column.embedding_column(X_loc, NUM_BUCKETS),\n    tf.feature_column.numeric_column('longitude_dist'),\n    tf.feature_column.numeric_column('latitude_dist'),\n    tf.feature_column.numeric_column('distance'),\n    tf.feature_column.numeric_column('is_JFK_pickup'),\n    tf.feature_column.numeric_column('is_JFK_dropoff'),\n    tf.feature_column.numeric_column('is_EWR_pickup'),\n    tf.feature_column.numeric_column('is_EWR_dropoff'),\n    tf.feature_column.numeric_column('is_LGU_pickup'),\n    tf.feature_column.numeric_column('is_LGU_dropoff'),\n    tf.feature_column.numeric_column('is_NYC_airport'),\n#    tf.feature_column.numeric_column('is_long_distance')\n  ]\n\n# Define your feature columns\ndef create_sparse_feature_cols():\n    return [\n    tf.feature_column.numeric_column('is_JFK_pickup'),\n    tf.feature_column.numeric_column('is_JFK_dropoff'),\n    tf.feature_column.numeric_column('is_EWR_pickup'),\n    tf.feature_column.numeric_column('is_EWR_dropoff'),\n    tf.feature_column.numeric_column('is_LGU_pickup'),\n    tf.feature_column.numeric_column('is_LGU_dropoff'),\n    tf.feature_column.numeric_column('is_NYC_airport'),\n#    tf.feature_column.numeric_column('is_long_distance')\n  ]\n\ndef create_dense_feature_cols():\n    hour_cat = tf.feature_column.categorical_column_with_identity('pickup_hour', 24 )\n    weekday_cat = tf.feature_column.categorical_column_with_identity('pickup_weekday', 7)\n    hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n    month_cat = tf.feature_column.categorical_column_with_identity('pickup_month', 13 )\n    return [\n    tf.feature_column.embedding_column(hour_X_weekday, 2),\n    tf.feature_column.embedding_column(month_cat, 2),\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    #TODO use pickup_year\n    tf.feature_column.numeric_column('pickup_dense_year'),\n#    tf.feature_column.numeric_column('pickup_year'),\n#    tf.feature_column.numeric_column('pickup_month'),\n#    tf.feature_column.numeric_column('pickup_day'),\n    #TODO use embeddings for the hour\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_hour', (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n    #                                                                        11, 12, 13, 14, 15, 16, 17, 18,\n    #                                                                         19, 20, 21, 22, 23) )\n    #                                  ),\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_weekday', (0, 1, 2, 3, 4, 5, 6)\n    #                                                                                            )),\n    tf.feature_column.numeric_column('longitude_dist'),\n    tf.feature_column.numeric_column('latitude_dist'),\n    tf.feature_column.numeric_column('distance'),\n  ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5607f77fb81893225f8e731f04d90919d00d2fd9"},"cell_type":"code","source":"BATCH_SIZE = 512\ntrain_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\n# Create estimator train and evaluate function\ndef train_and_evaluate(output_dir, num_train_steps):\n#    estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n    estimator = tf.estimator.DNNRegressor(model_dir = output_dir, feature_columns = create_feature_cols(),\n                                         hidden_units=[64, 64, 64])\n    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn, \n                                      max_steps = num_train_steps)\n    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn, \n                                    steps = None, \n                                    start_delay_secs = 600, # start evaluating after N seconds, \n                                    throttle_secs = 600)  # evaluate every N seconds\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n    return estimator\n    \n\nOUTDIR = './trained_model'\nshutil.rmtree(OUTDIR, ignore_errors = True)\n#model = train_and_evaluate(OUTDIR, NUM_STEPS)\n#print(\"BATCH SIZE = \", BATCH_SIZE,\"\\nDataset Take = \", 128*BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7394b8ab69057c7131c3761c08e82f923bb8c5ee","scrolled":false},"cell_type":"code","source":"BATCH_SIZE = 512\nOUTDIR = './trained_model'\ntrain_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\nshutil.rmtree(OUTDIR, ignore_errors = True)\n#estimator = tf.estimator.LinearRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols())\nestimator = tf.estimator.DNNRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols(),\n                                     hidden_units=[128, 128, 128], # 32 OK (43,37, 29: KO)\n                                     optimizer='Ftrl', \n                                     batch_norm=False, \n                                     dropout=0.1) \n                                     \n#estimator = tf.estimator.DNNLinearCombinedRegressor(model_dir = OUTDIR, \n#                                                    linear_feature_columns=create_sparse_feature_cols(),\n#                                                    dnn_feature_columns=create_dense_feature_cols(),\n#                                                    dnn_hidden_units=[128, 64, 32, 16],\n#                                                    dnn_dropout=None\n#                                                   )\nwith timer('Training...'):\n    estimator.train(train_input_fn, max_steps=80000)\nwith timer('Evaluating'):\n    evaluation = estimator.evaluate(eval_input_fn, name='train_eval')\nprint(evaluation)\nprint(evaluation, file = sys.stderr)\nprint(\"hidden_units=[64, 64, 64], V77\")\nprint(\"hidden_units=[64, 64, 64], V77\", file = sys.stderr)\nprint(\"Added/removed embedding_column(yearday, 2), V78\")\nprint(\"Added/removed embedding_column(yearday, 2), V78\", file = sys.stderr)\nprint(\"Added/removed clip fares at 2.50 instead of 1.0, V79\")\nprint(\"Added/removed log(fare_amount), V80\")\nprint(\"Added/removed hidden_units=[56, 56, 56], V82\")\nprint(\"Added embedded cross bucketized_column for lat/long, V85\")\nprint(\"First layer from 64 to 128, V88\")\nprint(\"Second layer from 64 to 128, V89\")\nprint(\"Third layer from 64 to 128, V90\")\nprint(\"max_steps=160000 from 80k, V91\")\nprint(\"reduced buckets amplitude, V92\")\nprint(\"num_buckets 11->20, V93\")\nprint(\"num_buckets 20->25, V96\")\nprint(\"num_buckets 25->27, V97\")\n#{'average_loss': 18.07","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0f0b1b2aa59ecaca8479a212fb3bb6859d0a7b1","_kg_hide-output":true},"cell_type":"code","source":"avg_loss = evaluation['average_loss']\npredict_input_fn = read_dataset(f'{PATH}/test.csv', tf.estimator.ModeKeys.PREDICT, batch_size=1)\npredictions = estimator.predict(predict_input_fn)\n\ntest_df = pd.read_csv(f'{PATH}/test.csv', nrows=10000)\n#test_df.head()\n\ns = pd.Series()\nfor i, p in enumerate(predictions):\n    if i < 9915:\n        s.at[i] = p['predictions'][0]\n    else:\n        break\ntest_df['fare_amount'] = s\nsub = test_df[['key', 'fare_amount']]\nsub.to_csv(f'DNNregr-{avg_loss:4.4}.csv', index=False)\n#    print(\"Prediction %s: %s\" % (i + 1, p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"424ec19726fd2aff6f4da3d3d5f2621bc3220869"},"cell_type":"code","source":"s.describe()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f36f10de598f7e56e65c0fd168f9eabf052a41d0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}